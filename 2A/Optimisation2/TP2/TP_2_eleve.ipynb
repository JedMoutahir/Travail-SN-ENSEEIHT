{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Méthodes d'optimisation stochastique\n",
    "\n",
    "## I. Minimisation stochastique d'une fonction déterministe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On s'intéresse au problème $\\min f(x) =\\max_{i=1\\dots m}|a_i^Tx-b_i | = \\max_{i=1\\dots m} |(Ax-b)_i|.$ \n",
    "\n",
    "On suppose que $a_i$ est un vecteur colonne représentant la $i$ eme ligne d'une matrice $A$ de taille $m \\times n$ ($m=100, n=20$) , et les $b_i$ sont les composantes d'un second membre $b$ de taille $m$, ($1 \\le i \\le m$), $x$ un vecteur de taille $n$. Ces quantités sont générées une fois pour toutes à partir de distributions Gaussiennes de moyenne nulle et d'écart type identité.\n",
    "\n",
    "**Question 1 :** Constuire $A$ et $b$. Proposer le calcul d'un sous-gradient en $x$ de $f$. On pourra utiliser la fonction findmax de Julia. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\partial f(x) = sgn(f(x)) \\times a_i$$\n",
    "Avec $i$ tel que $$f(x) = |(Ax-b)_i|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subgrad (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Construction des données A  et b\n",
    "# Insérer votre code\n",
    "m, n = 100, 20\n",
    "# Génération de la matrice A de taille mxn à partir de distributions gaussiennes de moyenne 0 et d' écart-type I\n",
    "A = randn(m, n)\n",
    "# Génération du vecteur b de taille m à partir de distributions gaussiennes de moyenne 0 et d' écart-type 1\n",
    "b = randn(m, 1)\n",
    "x = zeros(n, 1)\n",
    "# Fin insérer code\n",
    "\n",
    "# Fonction calculant la valeur de f en x\n",
    "function fval(A,b,x)\n",
    "    return maximum(abs.(A*x - b))\n",
    "end\n",
    "\n",
    "# Fonction calculant un sous-gradient en x de f\n",
    "function subgrad(A,b,x) \n",
    "    # Insérer votre code\n",
    "    # Calculer le sous-gradient de f en x\n",
    "    fx = abs.(A*x - b)\n",
    "    (~,ind) = findmax(fx)\n",
    "    i = ind[1]\n",
    "    subgrad = sign(fx[i]) .* (A[i,:])\n",
    "    return subgrad\n",
    "    # Fin insérer code\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 :** Ce problème peut se reformuler comme un problème de programmation linéaire : $$(\\mathcal{P}_{lp})\\quad \\left\\{ \\begin{array}{c} \\min_{(x,R)\\in \\mathbb{R}^n\\times \\mathbb{R}} h(x,R)=R\\\\\n",
    "s.c. \\quad-R*e\\leq A*x-b\\leq R*e\\end{array}\\right.$$ avec $e=[1,\\cdots,1]^T\\in \\mathbb{R}^m$. Résoudre le problème $(\\mathcal{P}_{lp})$ en utilisant le solveur \"GLPK\" de la librairie JuMP. Plus d'informations sont disponibles ici :  http://www.juliaopt.org/JuMP.jl/latest/quickstart/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Pkg\n",
    "#Pkg.add(\"JuMP\") \n",
    "#Pkg.add(\"GLPK\")\n",
    "using JuMP\n",
    "using GLPK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: GLPK not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: GLPK not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope at /home/jmoutahi/Bureau/2A/Optimisation2/TP2/TP_2_eleve.ipynb:5"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Définition du modèle\n",
    "# Insérer votre code\n",
    "model = Model(GLPK.Optimizer)\n",
    "# Fin insérer code\n",
    "\n",
    "# Définition des variables d'optimisation\n",
    "# Insérer votre code\n",
    "@variable(model, x[1:n])\n",
    "@variable(model, R)\n",
    "# Fin insérer code\n",
    "\n",
    "# Définition de la fonctionnelle à minimiser\n",
    "#Insérer votre code\n",
    "@objective(model, Min, R)\n",
    "# Fin insérer code\n",
    "\n",
    "# Définition des contraintes\n",
    "# Insérer votre code\n",
    "@constraint(model, c1[i=1:m], -R <= sum(A[i,j]*x[j] for j in 1:n) - b[i])\n",
    "@constraint(model, c2[i=1:m], R >= sum(A[i,j]*x[j] for j in 1:n) - b[i])\n",
    "# Fin insérer code\n",
    "        \n",
    "# Résolution        \n",
    "# Insérer votre code\n",
    "optimize!(model)\n",
    "# Fin insérer code\n",
    "        \n",
    "# Résultats à optimalité                \n",
    "# Insérer votre code\n",
    "println(solution_summary(model))\n",
    "xstar = value.(x)\n",
    "Rstar = value(R)\n",
    "\n",
    "# Fin insérer code\n",
    "println(\"The function value at the solution is: \",Rstar, \" or \",findmax(abs.(A*xstar-b)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 :** Résoudre le problème en utilisant un algorithme de sous-gradient. Dans un premier temps vous utiliserez un sous-gradient exact (Question 1), puis vous introduirez un bruit artificiel qui suit une distribution normale de moyenne nulle et d'écart-type $3 e-1$.\n",
    "\n",
    "**Question 4 :** Vous afficherez les courbes de convergence de $f_{best}^k-f_{star}$, avec $f_{star}$ obtenue à la Question 1. Donnez la valeur minimale de $f_{best}^k-f_{star}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pkg.add(\"Plots\")\n",
    "using Plots\n",
    "using LinearAlgebra\n",
    "\n",
    "#Initialisation\n",
    "x = zeros(n,1);\n",
    "i = 0;\n",
    "fbest =1e10; # $f_{best}^0$: cas du sous-gradient exact\n",
    "fbestp=1e10; # $f_{best}^0$: cas du sous-gradient bruité\n",
    "histo =[];# Suite des itérés f_{best}^k-f_{star} pour le cas du sous-gradient exact\n",
    "histop=[];# Suite des itérés f_{best}^k-f_{star}, pour le cas du sous-gradient bruité\n",
    "\n",
    "#Niveau de bruit\n",
    "noise_lvl=.3;\n",
    "\n",
    "# Resolution \n",
    "# Insérer votre code\n",
    "# Boucle sur les itérations\n",
    "iterMax = 100\n",
    "xk = x\n",
    "xpk = x\n",
    "for i in 1:iterMax\n",
    "\n",
    "    # Mise à jour de fbest\n",
    "    fbest = min(fbest,fval(A,b,xk))\n",
    "    fbestp = min(fbestp,fval(A,b,xpk))\n",
    "\n",
    "    # Calcul du sous-gradient exact\n",
    "    g = subgrad(A,b,xk)\n",
    "\n",
    "    # Calcul du sous-gradient bruité\n",
    "    gp = subgrad(A,b,xpk) + noise_lvl*randn(n,1)\n",
    "\n",
    "    # Calcul du pas optimal\n",
    "    alpha = 1e-2\n",
    "    alphap = 1e-2\n",
    "\n",
    "    # Mise à jour de x\n",
    "    xk = xk - alpha*g\n",
    "    xpk = xpk - alphap*gp\n",
    "\n",
    "\n",
    "    # Mise à jour des historiques\n",
    "    push!(histo,fbest-Rstar)\n",
    "    push!(histop,fbestp-Rstar)\n",
    "end\n",
    "# fbest value\n",
    "println(\"The function value at the solution is: \",fbest, \" or \",findmax(abs.(A*xk-b)))\n",
    "# Fin insérer code\n",
    "\n",
    "#Affichage des courbes de convergence\n",
    "iter=1:iterMax;\n",
    "hf=[histo,histop];\n",
    "plot(iter,hf,title=\"Convergence curves\",label=[\"Exact\" \"Noisy\"],lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## II. Minimisation stochastique d'une fonction stochatique\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "On s'intéresse au problème\n",
    "$$\\min_x f(x) =\\text{E} (\\max_{i=1\\dots m}|a_i^Tx-b_i |).$$\n",
    "\n",
    "On suppose que $a_i$ est un vecteur colonne représentant la $i$eme ligne d'une matrice $A$ de taille $m \\times n$ ($m=100, n=20$) , et les $b_i$ sont les composantes d'un second membre $b$ de taille $m$, ($1 \\le i \\le m$), $x$ un vecteur de taille $n$. Ces quantités sont générées une fois pour toutes à partir de distributions Gaussiennes de moyenne connue $\\bar{A}$ et $\\bar{b}$ (non nécesairement nulle) et d'écart type identité.\n",
    "\n",
    "**Question 5 :** Proposer deux fonctions d'évaluation de la fonction $f$ et d'un sous-gradient de $f$ basées sur des échantillons de taille $M$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation de f\n",
    "function fvals(Abar,bbar,noise,xs,M) \n",
    "    # Abar, bbar : moyenne des données\n",
    "    # noise : niveau de bruit \n",
    "    # xs : vecteur courant \n",
    "    # M: taille de l'échantillon\n",
    "\n",
    "# Insérer votre code\n",
    "sum = 0\n",
    "for i in 1:M\n",
    "    sum += fval(Abar + noise*randn(m,n),bbar + noise*randn(m,1),xs)\n",
    "end\n",
    "\n",
    "return sum/M\n",
    "# Fin insérer code\n",
    "\n",
    "end\n",
    "\n",
    "# Evaluation d'un sous-gradient\n",
    "function subgrads(Abar,bbar,noise,xs,M)\n",
    "    # Abar, bbar : moyenne des données\n",
    "    # noise : niveau de bruit \n",
    "    # xs : vecteur courant \n",
    "    # M: taille de l'échantillon\n",
    "    \n",
    "# Insérer votre code\n",
    "sum = zeros(n,1)\n",
    "for i in 1:M\n",
    "    sum += subgrad(Abar + noise*randn(m,n),bbar + noise*randn(m,1),xs)\n",
    "end\n",
    "\n",
    "return sum/M\n",
    "# Fin insérer code\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Question 6 :** Comparer les courbes de convergence du problème déterministe $$ \\min_x f(x) = \\max_{i=1\\dots m}|\\text{E} (a_i)^Tx-\\text{E} (b_i) |,$$ et du problème stochastique obtenu avec $M=10,100,1000$ échantillons. Donnez la valeur minimale de $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Données\n",
    "Abar=10*ones(m,n)+1*randn(m,n);\n",
    "bbar=10*randn(m,1);\n",
    "\n",
    "#x_0\n",
    "xd = zeros(n,1); # problème déterministe\n",
    "xs = xd; # problème stochastique\n",
    "\n",
    "# Bruit et echantillon\n",
    "M  = 200;\n",
    "noise  = 4;\n",
    "\n",
    "j = 0;\n",
    "\n",
    "fbestd =1e10; # $f_{best}^0$: cas d'une résolution déterministe\n",
    "fbests =1e10; # $f_{best}^0$: cas d'une résolution stochastique\n",
    "histod =[]; # Suite des itérés f_{best}^k pour le cas d'une résolution déterministe\n",
    "histos =[]; # Suite des itérés f_{best}^k pour le cas d'une résolution stochastique\n",
    "\n",
    "# Insérer votre code\n",
    "iterMax = 100\n",
    "for i in 1:iterMax\n",
    "\n",
    "    # Calcul du sous-gradient exact\n",
    "    gd = subgrad(Abar,bbar,xd)\n",
    "\n",
    "    # Calcul du sous-gradient bruité\n",
    "    gs = subgrads(Abar,bbar,noise,xs,M)\n",
    "\n",
    "    # Calcul du pas\n",
    "    alphad = 1e-2\n",
    "    alphas = 1e-2\n",
    "\n",
    "    # Mise à jour de x\n",
    "    xd = xd - alphad*gd\n",
    "    xs = xs - alphas*gs\n",
    "\n",
    "    # Mise à jour de fbest\n",
    "    fbestd = min(fbestd,fval(Abar,bbar,xd))\n",
    "    fbests = min(fbests,fvals(Abar,bbar,noise,xs,M))\n",
    "\n",
    "    # Mise à jour des historiques\n",
    "    push!(histod,fbestd-Rstar)\n",
    "    push!(histos,fbests-Rstar)\n",
    "end\n",
    "# Fin insérer code\n",
    "#Affichage\n",
    "\n",
    "iter=1:100;\n",
    "hf=[histod,histos];\n",
    "plot(iter,hf,title=\"Convergence curves\",label=[\"Deterministic\" \"Stochastic\"],lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Question 7 :** Répéter les expériences et comparer les valeurs meilleurs valeurs de f obtenues ($f_{best}$) aprs un nombre fixé d'itérations. Donnez la valeur minimale de $f_{best}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Données\n",
    "Abar=10*ones(m,n)+1*randn(m,n);\n",
    "bbar=10*randn(m,1);\n",
    "\n",
    "# x_0\n",
    "xd = zeros(n,1); # résolution déterministe\n",
    "xs = xd;  # résolution stochastique\n",
    "\n",
    "# Bruit et echantillon\n",
    "M  = 200;\n",
    "noise  = 4;\n",
    "\n",
    "#Nombre d'itérations\n",
    "niter=100;\n",
    "\n",
    "j = 0;\n",
    "\n",
    "fbestd =1e10; # $f_{best}^0$: cas d'une résolution déterministe\n",
    "fbests =1e10; # $f_{best}^0$: cas d'une résolution stochastique\n",
    "fbesttd=[]; # f_{best} pour chaque expériences dans le cas d'une résolution déterministe\n",
    "fbestts=[]; # f_{best} pour chaque expériences dans le cas d'une résolution stochastique\n",
    "\n",
    "for nexp=1:20\n",
    "    # Répétition des expériences    \n",
    "# Insérer votre code\n",
    "    for i in 1:niter\n",
    "\n",
    "        # Calcul du sous-gradient exact\n",
    "        gd = subgrad(Abar,bbar,xd)\n",
    "\n",
    "        # Calcul du sous-gradient bruité\n",
    "        gs = subgrads(Abar,bbar,noise,xs,M)\n",
    "\n",
    "        # Calcul du pas\n",
    "        alphad = 1e-2\n",
    "        alphas = 1e-2\n",
    "\n",
    "        # Mise à jour de x\n",
    "        xd = xd - alphad*gd\n",
    "        xs = xs - alphas*gs\n",
    "\n",
    "        # Mise à jour de fbest\n",
    "        fbestd = min(fbestd,fval(Abar,bbar,xd))\n",
    "        fbests = min(fbests,fvals(Abar,bbar,noise,xs,M))\n",
    "    end\n",
    "    push!(fbesttd,fbestd)\n",
    "    push!(fbestts,fbests)\n",
    "# Fin insérer code\n",
    "end\n",
    "\n",
    "#Affichage\n",
    "\n",
    "iter=1:20;\n",
    "hf=[fbesttd,fbestts];\n",
    "plot(iter,hf,title=\"Convergence curves\",label=[\"Deterministic\" \"Stochastic\"],lw=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
